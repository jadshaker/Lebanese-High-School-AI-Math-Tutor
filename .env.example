# API Keys
OPENAI_API_KEY=your_openai_api_key_here

# Small LLM Service Configuration (Ollama)
# Note: Use host.docker.internal for Docker, localhost for direct access
SMALL_LLM_SERVICE_URL=http://host.docker.internal:11434
SMALL_LLM_MODEL_NAME=deepseek-r1:7b

# Embedding Service Configuration
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSIONS=1536

# Fine-Tuned Model Service Configuration (Ollama)
# Note: Use host.docker.internal for Docker, localhost for direct access
FINE_TUNED_MODEL_SERVICE_URL=http://host.docker.internal:11434
FINE_TUNED_MODEL_NAME=tinyllama:latest

# Vector Cache Configuration
CACHE_TOP_K=5
QDRANT_HOST=qdrant
QDRANT_PORT=6333

# Confidence Thresholds (5-tier routing)
CONFIDENCE_TIER_1_THRESHOLD=0.95
CONFIDENCE_TIER_2_THRESHOLD=0.85
CONFIDENCE_TIER_3_THRESHOLD=0.70
CONFIDENCE_TIER_4_THRESHOLD=0.50

# Session Service Configuration
SESSION_TTL_SECONDS=3600
SESSION_MAX_HISTORY=50
SESSION_CLEANUP_INTERVAL=300

# Tutoring Configuration
TUTORING_ENABLED=true
TUTORING_MAX_DEPTH=4
TUTORING_INTERACTION_THRESHOLD=0.70

# Intent Classifier Configuration
INTENT_RULE_CONFIDENCE_THRESHOLD=0.80
INTENT_USE_LLM_FALLBACK=true

# Reformulator Configuration
REFORMULATOR_USE_LLM=true
