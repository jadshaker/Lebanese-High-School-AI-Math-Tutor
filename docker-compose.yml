services:
  # API Gateway - Main entry point (now using LangChain)
  gateway:
    build:
      context: services/gateway
      dockerfile: Dockerfile
    container_name: math-tutor-gateway
    ports:
      - '8000:8000'
    env_file:
      - .env
    environment:
      - EMBEDDING_SERVICE_URL=http://embedding:8002
      - OLLAMA_SERVICE_URL=http://host.docker.internal:11434
      - OLLAMA_MODEL_NAME=${OLLAMA_MODEL_NAME:-deepseek-r1:7b}
    command: uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload
    depends_on:
      - embedding
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - math-tutor-network

  # Large LLM Service (kept for backward compatibility, gateway now uses LangChain directly)
  large-llm:
    build:
      context: services/large_llm
      dockerfile: Dockerfile
    container_name: math-tutor-large-llm
    ports:
      - '8001:8001'
    env_file:
      - .env
    command: uvicorn src.main:app --host 0.0.0.0 --port 8001 --reload
    networks:
      - math-tutor-network

  # Small LLM Service (kept for backward compatibility, gateway now uses LangChain directly)
  small-llm:
    build:
      context: services/small_llm
      dockerfile: Dockerfile
    container_name: math-tutor-small-llm
    ports:
      - '8005:8005'
    env_file:
      - .env
    environment:
      - OLLAMA_SERVICE_URL=http://host.docker.internal:11434
      - OLLAMA_MODEL_NAME=${OLLAMA_MODEL_NAME:-deepseek-r1:7b}
    command: uvicorn src.main:app --host 0.0.0.0 --port 8005 --reload
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - math-tutor-network

  # Embedding Service
  embedding:
    build:
      context: services/embedding
      dockerfile: Dockerfile
    container_name: math-tutor-embedding
    ports:
      - '8002:8002'
    env_file:
      - .env
    command: uvicorn src.main:app --host 0.0.0.0 --port 8002 --reload
    networks:
      - math-tutor-network

networks:
  math-tutor-network:
    driver: bridge
