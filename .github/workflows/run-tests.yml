name: Run Tests with RunPod

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test-with-runpod:
    runs-on: ubuntu-latest
    env:
      SMALL_LLM_ENDPOINT_ID: ${{ vars.SMALL_LLM_ENDPOINT_ID }}
      REFORMULATOR_ENDPOINT_ID: ${{ vars.REFORMULATOR_ENDPOINT_ID }}
      FINE_TUNED_ENDPOINT_ID: ${{ vars.FINE_TUNED_ENDPOINT_ID }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.14'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create .env file
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          RUNPOD_API_KEY: ${{ secrets.RUNPOD_API_KEY }}
        run: |
          cat > .env << EOF
          OPENAI_API_KEY=$OPENAI_API_KEY

          SMALL_LLM_SERVICE_URL=https://api.runpod.ai/v2/${SMALL_LLM_ENDPOINT_ID}/openai
          SMALL_LLM_MODEL_NAME=deepseek-r1-7b
          SMALL_LLM_API_KEY=$RUNPOD_API_KEY

          REFORMULATOR_LLM_SERVICE_URL=https://api.runpod.ai/v2/${REFORMULATOR_ENDPOINT_ID}/openai
          REFORMULATOR_LLM_MODEL_NAME=deepseek-r1:7b
          REFORMULATOR_LLM_API_KEY=$RUNPOD_API_KEY

          EMBEDDING_MODEL=text-embedding-3-small
          EMBEDDING_DIMENSIONS=1536

          FINE_TUNED_MODEL_SERVICE_URL=https://api.runpod.ai/v2/${FINE_TUNED_ENDPOINT_ID}/openai
          FINE_TUNED_MODEL_NAME=deepseek-r1:7b
          FINE_TUNED_MODEL_API_KEY=$RUNPOD_API_KEY

          LARGE_LLM_MODEL_NAME=gpt-4o-mini

          CACHE_TOP_K=5

          INPUT_PROCESSOR_SERVICE_URL=http://input-processor:8004
          REFORMULATOR_SERVICE_URL=http://reformulator:8007
          EOF

          echo "✓ .env file created with RunPod Serverless endpoints"

      - name: Warm up RunPod endpoints
        env:
          RUNPOD_API_KEY: ${{ secrets.RUNPOD_API_KEY }}
        run: |
          echo "Warming up RunPod Serverless endpoints (triggers cold start)..."

          # Fire requests in parallel to warm up all endpoints simultaneously
          curl -s -m 300 "https://api.runpod.ai/v2/${SMALL_LLM_ENDPOINT_ID}/openai/v1/chat/completions" \
            -H "Authorization: Bearer $RUNPOD_API_KEY" \
            -H "Content-Type: application/json" \
            -d '{"model":"deepseek-r1-7b","messages":[{"role":"user","content":"hi"}]}' > /dev/null &
          PID1=$!

          curl -s -m 300 "https://api.runpod.ai/v2/${REFORMULATOR_ENDPOINT_ID}/openai/v1/chat/completions" \
            -H "Authorization: Bearer $RUNPOD_API_KEY" \
            -H "Content-Type: application/json" \
            -d '{"model":"deepseek-r1:7b","messages":[{"role":"user","content":"hi"}]}' > /dev/null &
          PID2=$!

          curl -s -m 300 "https://api.runpod.ai/v2/${FINE_TUNED_ENDPOINT_ID}/openai/v1/chat/completions" \
            -H "Authorization: Bearer $RUNPOD_API_KEY" \
            -H "Content-Type: application/json" \
            -d '{"model":"deepseek-r1:7b","messages":[{"role":"user","content":"hi"}]}' > /dev/null &
          PID3=$!

          # Wait for all warmup requests to complete
          wait $PID1 && echo "✓ Small LLM endpoint warm" || echo "⚠ Small LLM warmup failed"
          wait $PID2 && echo "✓ Reformulator endpoint warm" || echo "⚠ Reformulator warmup failed"
          wait $PID3 && echo "✓ Fine-tuned endpoint warm" || echo "⚠ Fine-tuned warmup failed"

          echo "✓ Warmup complete"

      - name: Start Docker Services
        run: |
          docker compose up -d
          echo "Waiting for services to start..."
          sleep 10

      - name: Check Service Health
        run: |
          echo "Waiting for all services to become healthy..."

          SERVICES=(
            "gateway:8000"
            "large-llm:8001"
            "embedding:8002"
            "cache:8003"
            "input-processor:8004"
            "small-llm:8005"
            "fine-tuned-model:8006"
            "reformulator:8007"
          )

          TIMEOUT=120
          INTERVAL=5

          for service in "${SERVICES[@]}"; do
            name="${service%:*}"
            port="${service#*:}"
            echo "Waiting for $name..."

            ELAPSED=0
            while [ $ELAPSED -lt $TIMEOUT ]; do
              HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" "http://localhost:$port/health" || echo "000")

              if [ "$HTTP_CODE" = "200" ]; then
                echo "  ✓ $name is healthy (HTTP $HTTP_CODE)"
                break
              fi

              echo "  Waiting for $name (HTTP $HTTP_CODE, elapsed: ${ELAPSED}s)..."
              sleep $INTERVAL
              ELAPSED=$((ELAPSED + INTERVAL))
            done

            if [ $ELAPSED -ge $TIMEOUT ]; then
              echo "  ✗ Timeout waiting for $name to become healthy"
              echo "Service logs:"
              docker compose logs --tail=50 "$name"
              exit 1
            fi
          done

          echo ""
          echo "✓ All services are healthy"

      - name: Run All Tests
        env:
          SMALL_LLM_MODEL_NAME: deepseek-r1-7b
          REFORMULATOR_LLM_MODEL_NAME: deepseek-r1:7b
          FINE_TUNED_MODEL_NAME: deepseek-r1:7b
          LARGE_LLM_MODEL_NAME: gpt-4o-mini
        run: |
          echo "Running all tests..."
          python3.14 -m pytest tests/ --dist loadgroup -n 4 -v --tb=short

      - name: Stop Docker Services
        if: always()
        run: |
          docker compose down
